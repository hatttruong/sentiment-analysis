{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# test_df = pd.read_csv(Path('dataset/test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(Path('dataset/train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>@manjulamartin @Kirk_Gleason Except trains are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>I want a Google driverless car.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>@Oatmeal @google driverless @TeslaMotors ? Ooo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>SO MUCH AWESOME! Amazing video for GoogleÌ¢‰âÂ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>@google is making driverless cars which is awe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          5  @manjulamartin @Kirk_Gleason Except trains are...\n",
       "1          5                    I want a Google driverless car.\n",
       "2          5  @Oatmeal @google driverless @TeslaMotors ? Ooo...\n",
       "3          5  SO MUCH AWESOME! Amazing video for GoogleÌ¢‰âÂ...\n",
       "4          5  @google is making driverless cars which is awe..."
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.text[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: remove emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_emojis(tweet):\n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMO_POS ', tweet)\n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)\\s', ' EMO_POS ', tweet)\n",
    "    # Love -- <3, :*\n",
    "    tweet = re.sub(r'(<3|:\\*)', ' EMO_POS ', tweet)\n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMO_POS ', tweet)\n",
    "    # Sad -- :-(, : (, :(, ):, )-:\n",
    "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMO_NEG ', tweet)\n",
    "    # Cry -- :,(, :'(, :\"(\n",
    "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMO_NEG ', tweet)\n",
    "    \n",
    "    tweet = tweet.replace(' EMO_POS ', ' ')\n",
    "    tweet = tweet.replace(' EMO_NEG ', ' ')\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_diff(before_df, after):\n",
    "    # check\n",
    "    count = 0\n",
    "    for index, row in before_df.iterrows():\n",
    "        if row['text'] != after[index]:\n",
    "    #         print(row['text'])\n",
    "    #         print(text[index])\n",
    "            count += 1\n",
    "    return count    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = train_df.processed_text.apply(handle_emojis)\n",
    "# count = check_diff(train_df, temp)\n",
    "# print('Number of tweets contains emoji: ', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.text = train_df.text.apply(handle_emojis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: split al URLS out off the main texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_url(tweet):\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets contains URL:  549\n"
     ]
    }
   ],
   "source": [
    "temp = train_df.text.apply(handle_url)\n",
    "count = check_diff(train_df, temp)\n",
    "print('Number of tweets contains URL: ', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.text = train_df.text.apply(handle_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: remove all non-ASCII characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_non_ASCII(tweet):\n",
    "    return ''.join([i if ord(i) < 128 else ' ' for i in tweet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets contains non-ASCII:  176\n"
     ]
    }
   ],
   "source": [
    "temp = train_df.text.apply(handle_non_ASCII)\n",
    "count = check_diff(train_df, temp)\n",
    "print('Number of tweets contains non-ASCII: ', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.text = train_df.text.apply(handle_non_ASCII)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: remove all numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_numbers(tweet):\n",
    "    # normal numbers\n",
    "    tweet = re.sub(r\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", tweet)\n",
    "    # money\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.text = train_df.text.apply(handle_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_df = pd.read_csv(Path('dataset/stopwords.csv'), header=None, names=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords =[row['text'] for index, row in stopwords_df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tweet):\n",
    "    tknzr = TweetTokenizer()\n",
    "    word_list = tknzr.tokenize(tweet)\n",
    "    word_list = [w for w in word_list if not w in stopwords]\n",
    "    return ' '.join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets contains stopwords:  980\n"
     ]
    }
   ],
   "source": [
    "temp = train_df.text.apply(remove_stopwords)\n",
    "count = check_diff(train_df, temp)\n",
    "print('Number of tweets contains stopwords: ', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.text = train_df.text.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: split out all hashtags and store all of the hashtags in txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hashtags(tweet):\n",
    "    hashtags = re.findall(r'#(\\S+)', tweet)\n",
    "    return tweet, hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_hashtags(df):\n",
    "    hashtags = list()\n",
    "    for index, row in df.iterrows():\n",
    "        tweet, ht = extract_hashtags(row['text'])\n",
    "        hashtags.extend(ht)\n",
    "        df.loc[index, 'text'] = tweet\n",
    "    return df, hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, hashtags = handle_hashtags(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hashtags.txt\", \"w\") as fp:\n",
    "    fp.write(\"\\n\".join(set(hashtags)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: print out 10 most popular hastags with counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('driverless', 30),\n",
       " ('google', 23),\n",
       " ('cars', 10),\n",
       " ('cas13', 8),\n",
       " ('codecon', 6),\n",
       " ('tech', 5),\n",
       " ('snbto', 4),\n",
       " ('toronto', 4),\n",
       " ('technology', 4),\n",
       " ('cbcmtl', 4)]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtags = [str.lower(h) for h in hashtags]\n",
    "c = collections.Counter(hashtags)\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8: remove all characters outside the alphabet system, except the whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alphabet(tweet):\n",
    "    regex = re.compile('[^a-zA-Z\\s]')\n",
    "    tweet = regex.sub('', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.text = train_df.text.apply(remove_non_alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9: combine all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(train_path):\n",
    "    train_df = pd.read_csv(Path(train_path))\n",
    "    train_df.text = train_df.text.apply(handle_emojis)\n",
    "    train_df.text = train_df.text.apply(handle_url)\n",
    "    train_df.text = train_df.text.apply(handle_non_ASCII)\n",
    "    train_df.text = train_df.text.apply(handle_numbers)\n",
    "    train_df.text = train_df.text.apply(remove_stopwords)\n",
    "    train_df, _ = handle_hashtags(train_df)\n",
    "    train_df.text = train_df.text.apply(remove_non_alphabet)\n",
    "    train_df.to_csv(Path(train_path + '.preprocess'), index=False)\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_df = preprocess('dataset/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def tokenize_and_strip(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        token = token.strip(string.punctuation + '…')\n",
    "        if len(token) == 1:\n",
    "            continue\n",
    "        if stemmer.stem(token) in english_vocab and 'sex' not in token:\n",
    "            continue\n",
    "        \n",
    "        # money/time\n",
    "        if re.match(r'(\\d{3,}k)', token) is not None:\n",
    "            continue\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cov_types=['spherical', 'diag', 'tied', 'full']\n",
    "# spherical gives bad results because it works like k-mean\n",
    "def gmm_clustering(n_components, X, y, stopwords, use_idf=True, cov_types=['diag', 'tied', 'full']):    \n",
    "    print('==========encode labels==========')\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    encoded_labels = le.fit_transform(y)\n",
    "    \n",
    "    actual_labels = le.inverse_transform(list(range(n_components)))\n",
    "    label2index = dict()\n",
    "    for i in range(n_components):\n",
    "        label2index[actual_labels[i]] = i\n",
    "        print('encoded label: %s, actual label: %s' % (i, actual_labels[i]))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        encoded_labels, test_size=0.33, random_state=42)\n",
    "        \n",
    "    print('==========calcuate tfidf matrix===========')\n",
    "    #define vectorizer parameters\n",
    "    # max_df=0.8, min_df=0.2, \n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=200000,\n",
    "                                       min_df=0.1, stop_words=stopwords,\n",
    "                                       use_idf=use_idf, tokenizer=tokenize_and_strip, ngram_range=(1, 3))\n",
    "    X_train = tfidf_vectorizer.fit_transform(X_train).toarray() #fit the vectorizer to synopses\n",
    "    print('X_train.shape: ', X_train.shape)\n",
    "    \n",
    "    X_test = tfidf_vectorizer.transform(X_test).toarray()\n",
    "    print('X_test.shape: ', X_test.shape)\n",
    "    \n",
    "    n_classes = n_components\n",
    "\n",
    "    # Try GMMs using different types of covariances.\n",
    "    estimators = dict((cov_type, GaussianMixture(n_components=n_classes,\n",
    "                       covariance_type=cov_type, max_iter=100, random_state=0))\n",
    "                      for cov_type in cov_types)\n",
    "\n",
    "    print('==========Cluster==========')\n",
    "    train_pred_probs = dict()\n",
    "    test_pred_probs = dict()\n",
    "    for index, (name, estimator) in enumerate(estimators.items()):\n",
    "        print('\\n--------Cov type: %s----------' % str.upper(name))\n",
    "        # Since we have class labels for the training data, we can\n",
    "        # initialize the GMM parameters in a supervised manner.\n",
    "        estimator.means_init = np.array([X_train[y_train == i].mean(axis=0)\n",
    "                                        for i in range(n_classes)])\n",
    "\n",
    "        # Train the other parameters using the EM algorithm.\n",
    "        estimator.fit(X_train)\n",
    "\n",
    "        y_train_pred = estimator.predict(X_train)\n",
    "        train_pred_probs[name] = estimator.predict_proba(X_train)\n",
    "        \n",
    "        train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel()) * 100\n",
    "        print('Train accuracy: %.1f' % train_accuracy)\n",
    "        for i in range(n_classes):\n",
    "            train_accuracy = np.mean(y_train_pred[y_train == i].ravel() == i) * 100\n",
    "            print('\\tclass-%s: %.1f' % (i, train_accuracy))\n",
    "            \n",
    "        print('Train Precision:')\n",
    "        for i in range(n_classes):\n",
    "            tp = sum(y_train_pred[y_train == i].ravel() == i)\n",
    "            train_precision = tp * 100. / sum(y_train_pred == i)\n",
    "            print('\\tclass-%s: %.1f' % (i, train_precision))\n",
    "            \n",
    "        print('Train Recall:')\n",
    "        for i in range(n_classes):\n",
    "            tp = sum(y_train_pred[y_train == i].ravel() == i)\n",
    "            train_recall = tp * 100. / sum(y_train == i)\n",
    "            print('\\tclass-%s: %.1f' % (i, train_recall))\n",
    "            \n",
    "        print('Confusion_matrix: \\n', confusion_matrix(y_train, y_train_pred))\n",
    "\n",
    "        y_test_pred = estimator.predict(X_test)\n",
    "        test_pred_probs[name] = estimator.predict_proba(X_test)\n",
    "        test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel()) * 100\n",
    "        print('\\nTest accuracy: %.1f' % test_accuracy)\n",
    "        for i in range(n_classes):\n",
    "            test_accuracy = np.mean(y_test_pred[y_test == i].ravel() == i) * 100\n",
    "            print('\\tclass-%s: %.1f' % (i, test_accuracy))\n",
    "            \n",
    "        print('Test Precision:')\n",
    "        for i in range(n_classes):\n",
    "            tp = sum(y_test_pred[y_test == i].ravel() == i)\n",
    "            test_precision = tp * 100. / sum(y_test_pred == i)\n",
    "            print('\\tclass-%s: %.1f' % (i, test_precision))\n",
    "            \n",
    "        print('Test Recall:')\n",
    "        for i in range(n_classes):\n",
    "            tp = sum(y_test_pred[y_test == i].ravel() == i)\n",
    "            test_recall = tp * 100. / sum(y_test == i)\n",
    "            print('\\tclass-%s: %.1f' % (i, test_recall))\n",
    "\n",
    "    return estimators, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = preprocess_df['text'].tolist()\n",
    "true_labels = preprocess_df['sentiment'].tolist()\n",
    "# for i in range(len(true_labels)):\n",
    "#     if true_labels[i] in [4, 5]:\n",
    "#         true_labels[i] = 'positive'\n",
    "#     elif true_labels[i] in [3]:\n",
    "#         true_labels[i] = 'neutral'\n",
    "#     else:\n",
    "#         true_labels[i] = 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "981"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative', 'neutral', 'positive'}"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========encode labels==========\n",
      "encoded label: 0, actual label: negative\n",
      "encoded label: 1, actual label: neutral\n",
      "encoded label: 2, actual label: positive\n",
      "==========calcuate tfidf matrix===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatruong/.local/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (657, 1)\n",
      "X_test.shape:  (324, 1)\n",
      "==========Cluster==========\n",
      "\n",
      "--------Cov type: DIAG----------\n",
      "Train accuracy: 19.9\n",
      "\tclass-0: 85.3\n",
      "\tclass-1: 0.0\n",
      "\tclass-2: 29.9\n",
      "Train Precision:\n",
      "\tclass-0: 15.7\n",
      "\tclass-1: nan\n",
      "\tclass-2: 35.7\n",
      "Train Recall:\n",
      "\tclass-0: 85.3\n",
      "\tclass-1: 0.0\n",
      "\tclass-2: 29.9\n",
      "Confusion_matrix: \n",
      " [[ 81   0  14]\n",
      " [319   0  76]\n",
      " [117   0  50]]\n",
      "\n",
      "Test accuracy: 16.7\n",
      "\tclass-0: 86.7\n",
      "\tclass-1: 0.0\n",
      "\tclass-2: 21.1\n",
      "Test Precision:\n",
      "\tclass-0: 15.5\n",
      "\tclass-1: nan\n",
      "\tclass-2: 20.8\n",
      "Test Recall:\n",
      "\tclass-0: 86.7\n",
      "\tclass-1: 0.0\n",
      "\tclass-2: 21.1\n",
      "\n",
      "--------Cov type: SPHERICAL----------\n",
      "Train accuracy: 19.9\n",
      "\tclass-0: 85.3\n",
      "\tclass-1: 0.0\n",
      "\tclass-2: 29.9\n",
      "Train Precision:\n",
      "\tclass-0: 15.7\n",
      "\tclass-1: nan\n",
      "\tclass-2: 35.7\n",
      "Train Recall:\n",
      "\tclass-0: 85.3\n",
      "\tclass-1: 0.0\n",
      "\tclass-2: 29.9\n",
      "Confusion_matrix: \n",
      " [[ 81   0  14]\n",
      " [319   0  76]\n",
      " [117   0  50]]\n",
      "\n",
      "Test accuracy: 16.7\n",
      "\tclass-0: 86.7\n",
      "\tclass-1: 0.0\n",
      "\tclass-2: 21.1\n",
      "Test Precision:\n",
      "\tclass-0: 15.5\n",
      "\tclass-1: nan\n",
      "\tclass-2: 20.8\n",
      "Test Recall:\n",
      "\tclass-0: 86.7\n",
      "\tclass-1: 0.0\n",
      "\tclass-2: 21.1\n",
      "\n",
      "--------Cov type: TIED----------\n",
      "Train accuracy: 19.9\n",
      "\tclass-0: 85.3\n",
      "\tclass-1: 0.0\n",
      "\tclass-2: 29.9\n",
      "Train Precision:\n",
      "\tclass-0: 15.7\n",
      "\tclass-1: nan\n",
      "\tclass-2: 35.7\n",
      "Train Recall:\n",
      "\tclass-0: 85.3\n",
      "\tclass-1: 0.0\n",
      "\tclass-2: 29.9\n",
      "Confusion_matrix: \n",
      " [[ 81   0  14]\n",
      " [319   0  76]\n",
      " [117   0  50]]\n",
      "\n",
      "Test accuracy: 16.7\n",
      "\tclass-0: 86.7\n",
      "\tclass-1: 0.0\n",
      "\tclass-2: 21.1\n",
      "Test Precision:\n",
      "\tclass-0: 15.5\n",
      "\tclass-1: nan\n",
      "\tclass-2: 20.8\n",
      "Test Recall:\n",
      "\tclass-0: 86.7\n",
      "\tclass-1: 0.0\n",
      "\tclass-2: 21.1\n",
      "\n",
      "--------Cov type: FULL----------\n",
      "Train accuracy: 19.9\n",
      "\tclass-0: 85.3\n",
      "\tclass-1: 0.0\n",
      "\tclass-2: 29.9\n",
      "Train Precision:\n",
      "\tclass-0: 15.7\n",
      "\tclass-1: nan\n",
      "\tclass-2: 35.7\n",
      "Train Recall:\n",
      "\tclass-0: 85.3\n",
      "\tclass-1: 0.0\n",
      "\tclass-2: 29.9\n",
      "Confusion_matrix: \n",
      " [[ 81   0  14]\n",
      " [319   0  76]\n",
      " [117   0  50]]\n",
      "\n",
      "Test accuracy: 16.7\n",
      "\tclass-0: 86.7\n",
      "\tclass-1: 0.0\n",
      "\tclass-2: 21.1\n",
      "Test Precision:\n",
      "\tclass-0: 15.5\n",
      "\tclass-1: nan\n",
      "\tclass-2: 20.8\n",
      "Test Recall:\n",
      "\tclass-0: 86.7\n",
      "\tclass-1: 0.0\n",
      "\tclass-2: 21.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:61: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:83: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "estimators, tfidf_vectorizer = gmm_clustering(n_components=3, X=contents, y=true_labels, \n",
    "                                              stopwords=stopwords, cov_types=['spherical', 'diag', 'tied', 'full'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that GMM model does not works well on this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_path, estimator, tfidf_vectorizer):\n",
    "    preprocess_df = preprocess(test_path)\n",
    "    contents = preprocess_df['text'].tolist()\n",
    "    X_train = tfidf_vectorizer.fit_transform(X_train).toarray() #fit the vectorizer to synopses\n",
    "    result = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict('dataset/test_path.csv', estimators['diag'], tfidf_vectorizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
