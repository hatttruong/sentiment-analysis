{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import collections\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train doc2vec\n",
    "- using DBOW with dm_concat=1 give better result than using DM with dm_concat=1\n",
    "- DM, concat: \n",
    "    + word vector:\n",
    "    + sentence vector:\n",
    "    \n",
    "- DBOW, concat: \n",
    "    + word vector: bad\n",
    "    + sentence vector:\n",
    "\n",
    "- DM, sum: \n",
    "    + word vector: ok\n",
    "    + sentence vector: average\n",
    "    \n",
    "- DBOW, sum: \n",
    "    + word vector: bad\n",
    "    + sentence vector: good\n",
    "\n",
    "- DM, mean: \n",
    "    + word vector: ok\n",
    "    + sentence vector: average\n",
    "    \n",
    "- DBOW, mean:\n",
    "    + word vector: bad\n",
    "    + sentence vector: good?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " models = {\n",
    "        'dm_concat': None,\n",
    "#         'dbow_concat': None,\n",
    "#         'dm_sum': None,\n",
    "#         'dbow_sum': None,\n",
    "#         'dm_mean': None,\n",
    "#         'dbow_mean': None\n",
    "    }\n",
    "pattern = '../model/doc2vec/doc2vec_external_%s.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "361003"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load all documents\n",
    "def iterdocuments(filenames, tokens_only=False):\n",
    "    \"\"\"\n",
    "    Iterate over documents, yielding a list of utf8 tokens at a time.\n",
    "\n",
    "    Args:\n",
    "        filenames (TYPE): Description\n",
    "        encoding (str, optional): Description\n",
    "\n",
    "    Yields:\n",
    "        TYPE: Description\n",
    "    \"\"\"\n",
    "    index = 0\n",
    "    for filename in filenames:\n",
    "        df = pd.read_csv(filename, names=['text'])\n",
    "        for _, row in df.iterrows():\n",
    "            if tokens_only:\n",
    "                yield gensim.utils.simple_preprocess(row['text'])\n",
    "            else:\n",
    "                yield gensim.models.doc2vec.TaggedDocument(\n",
    "                    gensim.utils.simple_preprocess(row['text']), [index])\n",
    "                index += 1\n",
    "\n",
    "train_corpus = list(iterdocuments(['../data/sentences_data.csv']))\n",
    "len(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in models.keys():\n",
    "    models[k] = pattern % k\n",
    "    model = gensim.models.doc2vec.Doc2Vec.load(models[k])\n",
    "    print('\\nMODEL:', k)\n",
    "    print('Number of documents:', len(model.docvecs))\n",
    "    # get similarity by words\n",
    "    compared_words = ['ram', 'bộ_nhớ', 'dung_lượng']\n",
    "    for w in compared_words:\n",
    "        print('Compare with \"%s\"' % w.upper())\n",
    "        similars = model.wv.similar_by_word(w, topn=10)\n",
    "        print('\\t' + ' '.join(['(%s, %.2f)' % (x[0], x[1]) for x in similars]))\n",
    "    \n",
    "    # get similar document\n",
    "    compared_document_ids = [69925, 69936, 83035]\n",
    "    for doc_id in compared_document_ids:\n",
    "        inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "        sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "        print('\\nDocument ({}): «{}»'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "        print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "        # for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "        #     print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))\n",
    "        for index in range(20):\n",
    "            print(u'%s: «%s»' % (sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([230804,  86509, 184852])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(1, high=len(train_corpus), size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.wv.similar_by_word('bộ_nhớ', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.wv.similar_by_word('dung_lượng', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(176697, 0.8649290800094604): «điện_thoại»\n",
      "(107748, 0.8645000457763672): «điện_thoại»\n",
      "(243107, 0.8614373803138733): «điện_thoại»\n",
      "(249427, 0.852385401725769): «điện_thoại wiko_sunny»\n",
      "(16070, 0.844206690788269): «điện_thoại»\n",
      "(243186, 0.8438059687614441): «điện_thoại»\n",
      "(245230, 0.8383656144142151): «điện_thoại masstel_fami»\n",
      "(244177, 0.837456226348877): «điện_thoại wiko_robby»\n",
      "(19986, 0.8273682594299316): «điện_thoại»\n",
      "(245338, 0.8181977272033691): «điện_thoại ivvi»\n",
      "(244220, 0.8088253140449524): «điện_thoại»\n",
      "(11866, 0.7841490507125854): «điện_thoại»\n",
      "(138124, 0.7770631909370422): «số điện_thoại»\n",
      "(60053, 0.7766164541244507): «số điện_thoại»\n",
      "(110019, 0.7734436392784119): «điện_thoại nhé»\n",
      "(8687, 0.7610524892807007): «google»\n",
      "(204653, 0.7505836486816406): «số điện_thoại»\n",
      "(79766, 0.7457122802734375): «tờ hi_chào bạn»\n",
      "(93906, 0.7446766495704651): «số điện_thoại»\n",
      "(145877, 0.7365298271179199): «điện_thoại của mình mua từ»\n"
     ]
    }
   ],
   "source": [
    "inferred_vector = model.infer_vector(['điện_thoại'])\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "for index in range(20):\n",
    "    print(u'%s: «%s»' % (sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document (251632): «bên cạnh đó bộ_nhớ trong với dung_lượng gb sẽ đáp_ứng tốt nhu_cầu lưu_trữ dữ_liệu của bạn»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dbow,d400,n5,hs,mc5,s0.0001,t3):\n",
      "\n",
      "(251632, 0.7037237882614136): «bên cạnh đó bộ_nhớ trong với dung_lượng gb sẽ đáp_ứng tốt nhu_cầu lưu_trữ dữ_liệu của bạn»\n",
      "(243956, 0.5447922945022583): «bên cạnh đó cool dual còn được trang_bị bộ_nhớ trong dung_lượng lên đến gb cho phép người dùng thoải_mái lưu_trữ những dữ_liệu cần_thiết»\n",
      "(246367, 0.5384358167648315): «máy cũng hỗ_trợ thẻ_nhớ dung_lượng gb giúp bạn mở_rộng bộ_nhớ để lưu_trữ dữ_liệu hình_ảnh»\n",
      "(247378, 0.5149009227752686): «thêm vào đó bộ_nhớ trong gb cho bạn thỏa_sức lưu_trữ dữ_liệu hình_ảnh»\n",
      "(244982, 0.5140997171401978): «bộ_nhớ trong gb giúp người dùng thoải_mái lưu_trữ dữ_liệu phục_vụ giải_trí và công_việc ngoài_ra máy còn trang_bị khe cắm thẻ_nhớ microsd hỗ_trợ lên tới gb đáp_ứng các nhu_cầu cao hơn của người dùng»\n",
      "(244754, 0.5128475427627563): «bên cạnh đó máy có bộ_nhớ ram gb cùng bộ_nhớ trong đến gb hỗ_trợ thẻ_nhớ microsd tối_đa gb cho bạn thoải_mái lưu_trữ hình_ảnh dữ_liệu mà không cần lo_lắng về vấn_đề dung_lượng»\n",
      "(247983, 0.5091269016265869): «thiết_bị này sở_hữu dung_lượng ram gb dung_lượng bộ_nhớ trong lên đến gb và có hỗ_trợ khe cắm thẻ_nhớ lên đến gb đảm_bảo không_gian lưu_trữ dữ_liệu hay các ứng_dụng thú_vị nhất cho người dùng»\n",
      "(247554, 0.50766921043396): «bộ_nhớ trong gb giúp người dùng thoải_mái lưu_trữ dữ_liệu phục_vụ giải_trí và công_việc ngoài_ra máy còn trang_bị khe cắm thẻ_nhớ microsd hỗ_trợ lên tới gb đáp_ứng các nhu_cầu cao hơn của người dùng»\n",
      "(134112, 0.5063415169715881): «đang dùng đây»\n",
      "(243916, 0.5051079988479614): «on chip helio nhân bit ram gb cùng bộ_nhớ trong lớn gb cho bạn thoải_mái cài_đặt game ứng_dụng và lưu_trữ dữ_liệu»\n",
      "(246829, 0.5029787421226501): «thêm vào đó bộ_nhớ trong gb cho bạn thỏa_sức lưu_trữ dữ_liệu hình_ảnh»\n",
      "(249614, 0.501954197883606): «bên cạnh đó với bộ_nhớ trong gb chiếc điện_thoại xiaomi giúp người dùng lưu_trữ được rất nhiều dữ_liệu cũng như thoải_mái chụp ảnh quay video»\n",
      "(249512, 0.4999573230743408): «dung_lượng gb không quá lớn nhưng vẫn thừa sức lưu_trữ thoải_mái dữ_liệu người dùng»\n",
      "(245286, 0.49389398097991943): «bên cạnh đó với bộ_nhớ trong gb ck giúp người dùng lưu_trữ được rất nhiều dữ_liệu cũng như thoải_mái chụp ảnh quay video»\n",
      "(248581, 0.4936707019805908): «bộ_nhớ trong gb hỗ_trợ khe cắm thẻ_nhớ với dung_lượng tối_đa lên tới gb giúp bạn thoải_mái lưu_trữ dữ_liệu»\n",
      "(248855, 0.4929311275482178): «máy có bộ_nhớ trong với dung_lượng gb khả_dụng khoảng gb để bạn có_thể thoải_mái lưu_trữ dữ_liệu cài_đặt ứng_dụng»\n",
      "(251660, 0.4924774765968323): «sản_phẩm có bộ_nhớ trong với dung_lượng gb để bạn thoải_mái lưu_trữ dữ_liệu hoặc cài_đặt ứng_dụng máy còn hỗ_trợ thẻ_nhớ»\n",
      "(246558, 0.4923928380012512): «dung_lượng bộ_nhớ lớn»\n",
      "(248869, 0.489737868309021): «model gr»\n",
      "(243311, 0.48633450269699097): «dung_lượng bộ_nhớ trong gb»\n",
      "(197137, 0.4847089350223541): «tuy_nhiên tin_tặc không lấy hay xóa bất_kỳ dữ_liệu nào»\n",
      "(250424, 0.48450514674186707): «model it»\n",
      "(244723, 0.4841633439064026): «bộ_nhớ trong của máy với dung_lượng gb và hỗ_trợ khe cắm»\n",
      "(246393, 0.48128342628479004): «bộ_nhớ trong gb đủ để đáp_ứng nhu_cầu cài ứng_dụng game và lưu_trữ hình_ảnh video clip của bạn»\n",
      "(250309, 0.48127618432044983): «bộ_nhớ trong gb giúp người dùng thoải_mái lưu_trữ dữ_liệu phục_vụ giải_trí và công_việc ngoài_ra máy còn trang_bị khe cắm»\n",
      "(250365, 0.4808024764060974): «bên cạnh đó với bộ_nhớ trong gb chiếc điện_thoại xiaomi giúp người dùng lưu_trữ được rất nhiều dữ_liệu cũng như thoải_mái chụp ảnh quay video»\n",
      "(48878, 0.4787997901439667): «thẻ_nhớ có dung_lượng lên đến gb cho người dùng thoải_mái lưu_trữ nhiều hình_ảnh hơn đồng_thời giúp nâng cao hiệu_suất làm_việc nhờ công_nghệ power core để truyền_tải dữ_liệu nhanh_chóng và hiệu_quả»\n",
      "(110923, 0.4778776466846466): «mobi dùng ko giới_hạn dung_lượng»\n",
      "(243821, 0.4772316813468933): «với khe cắm sim masstel izi giúp người dùng quản_lý liên_lạc tối_ưu chi_phí cước viễn_thông một_cách hiệu_quả»\n",
      "(159297, 0.4765685498714447): «vãi_l_t đang dùng»\n"
     ]
    }
   ],
   "source": [
    "doc_id = 251632\n",
    "inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "# for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "#     print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))\n",
    "for index in range(30):\n",
    "    print(u'%s: «%s»' % (sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing Model\n",
    "ranks = []\n",
    "second_ranks = []\n",
    "total = len(train_corpus)\n",
    "max_length = 30\n",
    "pattern = \"[%-30s] %d%%\"\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "    \n",
    "    second_ranks.append(sims[1])\n",
    "    sys.stdout.write('\\r'\n",
    "    # the exact output you're looking for\n",
    "    frac = int((doc_id + 1) * 100. / total)\n",
    "    cur_length = int(frac * max_length * 1. / 100)\n",
    "    sys.stdout.write(pattern % ('=' * cur_length, frac))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections.Counter(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert documents to vector\n",
    "def iterdocuments_to_vector(filenames, vectorizer_model):\n",
    "    \"\"\"\n",
    "    Iterate over documents, yielding vector at a time.\n",
    "\n",
    "    Args:\n",
    "        filenames (TYPE): Description\n",
    "        \n",
    "    Yields:\n",
    "        TYPE: Description\n",
    "    \"\"\"\n",
    "    for filename in filenames:\n",
    "        df = pd.read_csv(filename, header=None, names=['text'])\n",
    "        logger.info('corpus: %s, size: %s',\n",
    "                    os.path.basename(filename),\n",
    "                    df.shape[0])\n",
    "        for _, row in df.iterrows():\n",
    "            yield vectorizer_model.infer_vector(row['text'].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster documents of Organizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of document is too large, we cannot use Affinity Propagation\n",
    "# option 1: use Kmean\n",
    "# option 2: use hierarchy clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([model.docvecs[i] for i in range(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1: use Kmean\n",
    "kmeans_model = KMeans(n_clusters=300, random_state=1).fit(X)\n",
    "labels = kmeans_model.labels_\n",
    "metrics.calinski_harabaz_score(X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2: use hierarchy clustering\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
